<!doctype html><html lang=en itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel=icon href=/favicon.svg><title>CogVideoX:A Cutting-Edge Video Generation Models - GLM
</title><meta name=description content="As a highly complex system, human cognitive function relies on the collaborative work between various regions of the brain, which involves not only the processing of text and language, but also multiple aspects such as visual understanding and auditory processing.
We firmly believe that the integration and improvement of perception and understanding in Multi-modal Learning is closely related to the development of cognitive abilities.
As a company dedicated to achieving Artificial General Intelligence (AGI), ZhipuAI has always attached great importance to the development of MultiModal Machine Learning technology."><meta name=author content="zhpu AI"><meta name=generator content="Hugo 0.131.0"><link rel=stylesheet href="https://2404589803.github.io/css/styles.min.604fd6ddfbcfaf51795dc18d21ded1d9f7eadf8b91a4651731bdbacc8585608d.css" integrity="sha256-YE/W3fvPr1F5XcGNId7R2ffq34uRpGUXMb26zIWFYI0="><meta property="og:url" content="https://2404589803.github.io/en/blog/cogvideox----a-cutting-edge-video-generation-model/"><meta property="og:site_name" content="GLM"><meta property="og:title" content="CogVideoX:A Cutting-Edge Video Generation Models"><meta property="og:description" content="As a highly complex system, human cognitive function relies on the collaborative work between various regions of the brain, which involves not only the processing of text and language, but also multiple aspects such as visual understanding and auditory processing.
We firmly believe that the integration and improvement of perception and understanding in Multi-modal Learning is closely related to the development of cognitive abilities.
As a company dedicated to achieving Artificial General Intelligence (AGI), ZhipuAI has always attached great importance to the development of MultiModal Machine Learning technology."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-07-26T00:00:00+00:00"><meta property="article:modified_time" content="2024-07-26T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="CogVideoX:A Cutting-Edge Video Generation Models"><meta name=twitter:description content="As a highly complex system, human cognitive function relies on the collaborative work between various regions of the brain, which involves not only the processing of text and language, but also multiple aspects such as visual understanding and auditory processing.
We firmly believe that the integration and improvement of perception and understanding in Multi-modal Learning is closely related to the development of cognitive abilities.
As a company dedicated to achieving Artificial General Intelligence (AGI), ZhipuAI has always attached great importance to the development of MultiModal Machine Learning technology."><meta itemprop=name content="CogVideoX:A Cutting-Edge Video Generation Models"><meta itemprop=description content="As a highly complex system, human cognitive function relies on the collaborative work between various regions of the brain, which involves not only the processing of text and language, but also multiple aspects such as visual understanding and auditory processing.
We firmly believe that the integration and improvement of perception and understanding in Multi-modal Learning is closely related to the development of cognitive abilities.
As a company dedicated to achieving Artificial General Intelligence (AGI), ZhipuAI has always attached great importance to the development of MultiModal Machine Learning technology."><meta itemprop=datePublished content="2024-07-26T00:00:00+00:00"><meta itemprop=dateModified content="2024-07-26T00:00:00+00:00"><meta itemprop=wordCount content="564"></head><body class="dark:bg-gray-800 dark:text-white relative flex flex-col min-h-screen"><!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Document</title>
<link href=https://cdn.jsdelivr.net/npm/tailwindcss@3.2.4/dist/tailwind.min.css rel=stylesheet><style>.header-container{transition:background-color .3s,color .3s}.language-dropdown{display:none}.language-switcher:hover .language-dropdown{display:flex}</style></head><body><header class="header-container container flex justify-between items-center p-6 mx-auto relative sticky top-0 z-50 bg-white"><a href=https://2404589803.github.io/en/ class="header-title capitalize font-extrabold text-2xl"><img src=/blist-logo.png alt=GLM class="h-8 max-w-full"></a><ul class="flex items-center gap-4 lg:gap-6"><li><a href=/en/>Home</a></li><li><a href=/en/blog>Blog</a></li><li><a href=/en/page/about/>About</a></li><div class="bg-white dark:bg-gray-900"><div><div class="text-2xl font-bold mb-2"></div><p class=opacity-60></p></div><ul class="flex justify-center gap-x-3 flex-wrap gap-y-2"><li><a href=https://twitter.com/ target=_blank rel=noopener aria-label=Twitter class="p-1 inline-block rounded-full border border-transparent text-gray-500 hover:text-gray-800 hover:border-gray-800 cursor-pointer transition-colors dark:text-gray-600 dark:hover:border-gray-300 dark:hover:text-gray-300"><svg width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li><li><a href=https://github.com/ target=_blank rel=noopener aria-label=GitHub class="p-1 inline-block rounded-full border border-transparent text-gray-500 hover:text-gray-800 hover:border-gray-800 cursor-pointer transition-colors dark:text-gray-600 dark:hover:border-gray-300 dark:hover:text-gray-300"><svg width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.334 1.333-4.334-2.667-6-3m12 3v-2.26A3.5 3.5.0 0014.06 14c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0019 3.19a5.07 5.07.0 00-.09-3.81s-1.18-.35-3.91 1.48a13.38 13.38.0 00-7.02.0c-2.73-1.83-3.91-1.48-3.91-1.48a5.07 5.07.0 00-.09 3.81A5.44 5.44.0 002.48 7c0 5.42 3.3 6.65 6.44 7a3.5 3.5.0 00-.94 2.74V19"/></svg></a></li><li><a href=https://youtube.com target=_blank rel=noopener aria-label=YouTube class="p-1 inline-block rounded-full border border-transparent text-gray-500 hover:text-gray-800 hover:border-gray-800 cursor-pointer transition-colors dark:text-gray-600 dark:hover:border-gray-300 dark:hover:text-gray-300"><svg width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="3" y="5" width="18" height="14" rx="4"/><path d="M10 9l5 3-5 3z"/></svg></a></li></ul></div></ul></header><script>document.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".header-container");window.addEventListener("scroll",()=>{window.scrollY>50?e.classList.add("bg-white"):e.classList.remove("bg-white")})})</script></body></html><main class=flex-1><div class="relative max-w-5xl mx-auto px-4"><img src=/cogvideox.png class="rounded-lg shadow-sm w-full object-contain"><div class="absolute top-4 right-8 rounded shadow bg-white text-gray-900 dark:bg-gray-900 dark:text-white px-2 py-0.5">July 26, 2024</div></div><article class="prose lg:prose-lg mx-auto my-8 dark:prose-dark px-4"><h1 class="text-2xl font-bold mb-2">CogVideoX:A Cutting-Edge Video Generation Models</h1><h5 class="text-sm flex items-center flex-wrap"><svg class="mr-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="4" y="5" width="16" height="16" rx="2"/><line x1="16" y1="3" x2="16" y2="7"/><line x1="8" y1="3" x2="8" y2="7"/><line x1="4" y1="11" x2="20" y2="11"/><rect x="8" y="15" width="2" height="2"/></svg>
Posted on
July 26, 2024
&nbsp;&bull;&nbsp;
<svg class="mr-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
3&nbsp;minutes
&nbsp;&bull;
<svg class="mx-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 19a9 9 0 019 0 9 9 0 019 0"/><path d="M3 6a9 9 0 019 0 9 9 0 019 0"/><line x1="3" y1="6" x2="3" y2="19"/><line x1="12" y1="6" x2="12" y2="19"/><line x1="21" y1="6" x2="21" y2="19"/></svg>
564&nbsp;words</h5><p>As a highly complex system, human cognitive function relies on the collaborative work between various regions of the brain, which involves not only the processing of text and language, but also multiple aspects such as visual understanding and auditory processing.</p><p>We firmly believe that the integration and improvement of perception and understanding in Multi-modal Learning is closely related to the development of cognitive abilities.</p><p>As a company dedicated to achieving Artificial General Intelligence (AGI), ZhipuAI has always attached great importance to the development of MultiModal Machine Learning technology. Since 2021, the ZPAI technology team has been laying out MultiModal Machine Learning models including text-2-img, text-2-video, img-2-text, and video-2-text, and has successively developed and open-sourced multiple advanced models
such as CogView, CogVideo, Relay Diffusion, CogVLM, and CogVLM-Video.</p><p>Here, we are pleased to announce that the video generative model has been upgraded and the new generation product - CogVideoX has been officially launched.</p><p>The core technological features of CogVideoX are as follows:</p><p>Addressing the issue of content coherence, Zhipu AI has independently developed an efficient 3D Variational Autoencoder (3D VAE) structure. This structure is capable of compressing raw video data to 2% of its original size, significantly reducing the training cost and difficulty for video diffusion generation models. Combined with the 3D RoPE position encoding module, this technology effectively enhances the capturing ability of frame relationships over time, thereby establishing long-term dependencies within videos.
In terms of controllability, Zhipu AI has created an end-to-end video understanding model that can generate precise and content-related descriptions for a large amount of video data. This innovation strengthens the model&rsquo;s comprehension of text and its ability to follow instructions, ensuring that the generated videos are more aligned with user input requirements and can handle overly long and complex prompt instructions.</p><p>Our model adopts a transformer architecture that integrates text, time, and space into a single three-dimensional fusion. This architecture abandons the traditional cross-attention module and innovatively designs an Expert Block to achieve alignment between the text and video modalities. It further optimizes the interaction effects between modalities through a Full Attention mechanism.</p><p>CogVideoX has been officially launched on Zhipu Qingyan&rsquo;s PC, mobile application, and mini-program platforms. All C-end users can experience the AI video generation feature &ldquo;Ying&rdquo;（清影） for free, which offers services for AI text-to-video and image-to-video generation through Zhipu Qingyan. (Link: <a href=https://chatglm.cn/video target=_blank rel=noopener>https://chatglm.cn/video</a>
)</p><p>The main features are as follows:</p><ol><li><strong>Quick Generate</strong>: Generate a 6-second video in just 30 seconds. Efficient instruction following ability: Even complex prompts, Qingying can accurately understand and execute.</li><li><strong>Content coherence</strong>: The generated video can better restore the motion process in the physical world.</li><li><strong>Picture scheduling flexibility</strong>: For example, the lens can smoothly follow the three dogs in the picture, just like a professional photographer&rsquo;s follow.</li></ol><p>In addition, we have also deployed &ldquo;Ying&rdquo; (清影) on the Zhipu Large Model Open Platform bigmodel.cn. Enterprises and developers can experience and use the text generation video and image generation video functions of &ldquo;Ying&rdquo;（清影） through API calls.</p><p><strong>DEMO</strong></p><p><strong>Text-to-Video</strong></p><p><strong>Example 1</strong></p><p><strong>Prompt</strong></p><p>Low-angle forward movement, slowly looking up, a dragon suddenly appears atop the iceberg, and then the dragon notices you and charges toward you. Hollywood movie style.</p><p><strong>Generated</strong></p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/fVetKRMMT64?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="YouTube video"></iframe></div><p><strong>Example 2</strong></p><p><strong>Prompt</strong></p><p>A mushroom transforms into a little bear.</p><p><strong>Generated</strong></p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/yYhZpq3LoVE?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="YouTube video"></iframe></div><p><strong>Example 3</strong></p><p><strong>Prompt</strong></p><p>Little yellow duck toy floating on the water in the swimming pool, close-up</p><p><strong>Generated</strong></p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/9GxJaMSEKnA?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="YouTube video"></iframe></div><p><strong>Example 4</strong></p><p><strong>Prompt</strong></p><p>Falling snowflakes, a little bird playing on the branch.</p><p><strong>Generated</strong></p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/XLBzjfm_cXk?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="YouTube video"></iframe></div></article><div class="px-2 mb-2"><script src=https://giscus.app/client.js data-repo data-repo-id data-category data-category-id data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></div></main><footer class="container p-6 mx-auto flex justify-between items-center"><span class="text-sm font-light">Copyright © 2024 - Zhipu AI · All rights reserved
</span><span onclick='window.scrollTo({top:0,behavior:"smooth"})' class="p-1 cursor-pointer"><svg width="30" height="30" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18 15l-6-6-6 6h12"/></svg></span></footer><script>const mobileMenuButton=document.querySelector(".mobile-menu-button"),mobileMenu=document.querySelector(".mobile-menu");function toggleMenu(){mobileMenu.classList.toggle("hidden"),mobileMenu.classList.toggle("flex")}mobileMenu&&mobileMenuButton&&mobileMenuButton.addEventListener("click",toggleMenu)</script></body></html>