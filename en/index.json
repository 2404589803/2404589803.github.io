[{"author":"Zhipu AI","contents":"We are a leading artificial intelligence company in China, focused on developing advanced AI technologies and applications. We specializes in natural language processing, machine learning, and data analysis, offering solutions that enhance business intelligence and operational efficiency. We provides a range of products and services, including AI-driven data platforms, intelligent customer service, and AI education platforms. We innovative solutions cater to various industries, aiming to drive digital transformation and improve decision-making processes.","permalink":"https://2404589803.github.io/en/page/about/","title":"About US"},{"author":"zhpu AI","contents":"As a highly complex system, human cognitive function relies on the collaborative work between various regions of the brain, which involves not only the processing of text and language, but also multiple aspects such as visual understanding and auditory processing.\nWe firmly believe that the integration and improvement of perception and understanding in Multi-modal Learning is closely related to the development of cognitive abilities.\nAs a company dedicated to achieving Artificial General Intelligence (AGI), ZhipuAI has always attached great importance to the development of MultiModal Machine Learning technology.","permalink":"https://2404589803.github.io/en/blog/cogvideox----a-cutting-edge-video-generation-model/","title":"CogVideoX:A Cutting-Edge Video Generation Models"},{"author":"zhpu AI","contents":"In early 2023, even the most advanced GPT-3.5 had a context length of only 2k. However, today, a context length of 1M has become one of the important indicators for measuring the technological advancement of models.\nIf LLM is compared to the operating system of the new era, the context window is its \u0026ldquo;memory\u0026rdquo;. A modern operating system requires sufficient memory to complete various complex tasks. Similarly, an excellent LLM also requires sufficient context length to complete various complex tasks.","permalink":"https://2404589803.github.io/en/blog/glm-long-----caling-pre-trained-model-contexts-to-millions/","title":"GLM Long: Scaling Pre-trained Model Contexts to Millions"}]